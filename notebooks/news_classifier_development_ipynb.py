# -*- coding: utf-8 -*-
"""News_Classifier_Development_ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CelS5VHw3IZlWv_oNXNgJLesr4JEHCi4
"""

# -*- coding: utf-8 -*-
"""
# üì∞ News Headline Classifier - Complete Implementation

**Objetivo:** Clasificar titulares de noticias usando m√∫ltiples enfoques de ML/LLM

**Modelos a probar:**
1. TF-IDF + Logistic Regression (baseline)
2. Sentence-Transformers + Random Forest (moderno)
3. Gemma LLM via Ollama (innovador)

**Dataset:** data.json con campos 'headline' y 'category'
"""

# ==================== 1. SETUP E INSTALACIONES ====================

# TODO --> Mis dependecias a utilizar
!pip install -q sentence-transformers scikit-learn pandas matplotlib seaborn plotly
!pip install -q wordcloud tqdm ollama requests
!curl -fsSL https://ollama.ai/install.sh | sh
!nohup ollama serve &
import time
time.sleep(5)  # Esperar a que se inicie Ollama
!ollama pull gemma:2b

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sentence_transformers import SentenceTransformer
import re
import string
from collections import Counter
import warnings
warnings.filterwarnings('ignore')


np.random.seed(42)
plt.style.use('default')

# ==================== 2. CARGA Y EDA DE DATOS ====================

def load_and_explore_data(file_path='/content/data.json'):
    """
    Esto sirve para cargar mi dataset y realizar mi analisis exploratirio
    Args:
        file_path (str): JSON con los datos "data.json"

    Returns:
        pd.DataFrame: DataFrame con los datos cargados
    """
    # Cargar datos
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    df = pd.DataFrame(data)

    print(f"Dataset con: {len(df)} registros")
    print(f"Columnas: {list(df.columns)}")
    print(f"Categor√≠as √∫nicas: {df['category'].nunique()}")

    print("\n" + "="*50)
    print("INFORME")
    print("="*50)
    print(df.info())

    print("\n" + "="*50)
    print("DISTRIBUCI√ìN DE CATEGOR√çAS")
    print("="*50)
    category_counts = df['category'].value_counts()
    print(category_counts)

    # Estad√≠sticas de texto
    df['headline_length'] = df['headline'].str.len()
    df['word_count'] = df['headline'].str.split().str.len()

    print("\n" + "="*50)
    print("ESTAD√çSTICAS DE TEXTO")
    print("="*50)
    print(f"Longitud promedio: {df['headline_length'].mean():.1f} caracteres")
    print(f"Palabras promedio: {df['word_count'].mean():.1f} palabras")

    # primeras visualizaciones
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # categor√≠as
    category_counts.plot(kind='bar', ax=axes[0,0])
    axes[0,0].set_title('Distribuci√≥n de Categor√≠as')
    axes[0,0].tick_params(axis='x', rotation=45)

    # Longitud de titulares
    df['headline_length'].hist(bins=30, ax=axes[0,1])
    axes[0,1].set_title('Distribuci√≥n de Longitud de Titulares')
    axes[0,1].set_xlabel('Caracteres')

    # N√∫mero de palabras
    df['word_count'].hist(bins=20, ax=axes[1,0])
    axes[1,0].set_title('Distribuci√≥n de N√∫mero de Palabras')
    axes[1,0].set_xlabel('Palabras')

    # longitud por categor√≠a
    df.boxplot(column='headline_length', by='category', ax=axes[1,1])
    axes[1,1].set_title('Longitud por Categor√≠a')
    axes[1,1].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

    return df

# ==================== 3. PREPROCESAMIENTO ====================

def preprocess_text(text):
    """
    Esta funcion sirve para el prepocesmiento clasificatorio.
    Args:
        text (str): Texto a preprocesar

    Returns:
        str: Texto limpio
    """
    if pd.isna(text):
        return ""


    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    text = ' '.join(text.split())

    return text

def prepare_data(df):
    """
    Preparacion de los datps para entrenar

    Args:
        df (pd.DataFrame):  datos originales

    Returns:
        tuple: X_train, X_test, y_train, y_test
    """

    df['headline_clean'] = df['headline'].apply(preprocess_text)

    X = df['headline_clean']
    y = df['category']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f" Datos preparados:")
    print(f"   Entrenamiento: {len(X_train)} muestras")
    print(f"   Prueba: {len(X_test)} muestras")
    print(f"   Categor√≠as: {len(y.unique())}")

    return X_train, X_test, y_train, y_test

# ==================== 4. MODELO 1: TF-IDF + LOGISTIC REGRESSION ====================

class TfidfClassifier:
    """Clasificador usando TF-IDF + Logistic Regression."""

    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            stop_words='english',
            min_df=2,
            max_df=0.95
        )
        self.model = LogisticRegression(random_state=42, max_iter=1000)
        self.is_trained = False

    def train(self, X_train, y_train):
        """Esta funcion entrena el modelo TF-IDF."""
        print("Entrenando modelo TF-IDF...")

        X_tfidf = self.vectorizer.fit_transform(X_train)

        self.model.fit(X_tfidf, y_train)
        self.is_trained = True

        print("Modelo TF-IDF entrenado")

    def predict(self, X_test):
        """Realiza predicciones."""
        if not self.is_trained:
            raise ValueError("Modelo no entrenado")

        X_tfidf = self.vectorizer.transform(X_test)
        predictions = self.model.predict(X_tfidf)
        probabilities = self.model.predict_proba(X_tfidf)

        return predictions, probabilities

    def predict_single(self, headline):
        """Predice una sola muestra."""
        if not self.is_trained:
            raise ValueError("Modelo no entrenado")

        clean_headline = preprocess_text(headline)
        X_tfidf = self.vectorizer.transform([clean_headline])
        prediction = self.model.predict(X_tfidf)[0]
        probabilities = self.model.predict_proba(X_tfidf)[0]

        # Crear diccionario de probabilidades
        prob_dict = dict(zip(self.model.classes_, probabilities))

        return {
            'category': prediction,
            'confidence': max(probabilities),
            'probabilities': prob_dict
        }

# ==================== 5. MODELO 2: SENTENCE TRANSFORMERS ====================

class SentenceTransformerClassifier:
    """Clasificador usando Sentence-Transformers + Random Forest."""

    def __init__(self):
        print("üì• Cargando Sentence-Transformer...")
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.is_trained = False
        print("Sentence-Transformer cargado")

    def train(self, X_train, y_train):
        """Entrena el modelo con embeddings."""
        print("Generando embeddings...")

        embeddings = self.encoder.encode(X_train.tolist(), show_progress_bar=True)

        print("üöÄ Entrenando Random Forest...")
        self.model.fit(embeddings, y_train)
        self.is_trained = True

        print("Modelo Sentence-Transformer entrenado")

    def predict(self, X_test):
        """Realiza predicciones."""
        if not self.is_trained:
            raise ValueError("Modelo no entrenado")

        embeddings = self.encoder.encode(X_test.tolist(), show_progress_bar=True)
        predictions = self.model.predict(embeddings)
        probabilities = self.model.predict_proba(embeddings)

        return predictions, probabilities

    def predict_single(self, headline):
        """Predice una sola muestra."""
        if not self.is_trained:
            raise ValueError("Modelo no entrenado")

        clean_headline = preprocess_text(headline)
        embedding = self.encoder.encode([clean_headline])
        prediction = self.model.predict(embedding)[0]
        probabilities = self.model.predict_proba(embedding)[0]

        prob_dict = dict(zip(self.model.classes_, probabilities))

        return {
            'category': prediction,
            'confidence': max(probabilities),
            'probabilities': prob_dict
        }

# ==================== 6. MODELO 3: GEMMA LLM ====================

import ollama

class GemmaClassifier:
    """Clasificador usando Gemma LLM."""

    def __init__(self, categories):
        self.categories = categories
        self.is_trained = True  # LLM no necesita entrenamiento

    def create_prompt(self, headline):
        """Esta funcion tiene mi prompt en ingles para mi clssificacion."""
        categories_str = ", ".join(self.categories)

        prompt = f"""Classify this news headline into ONE of these categories: {categories_str}

Headline: "{headline}"

Respond with ONLY the category name from the list above. Nothing else.

Examples:
- "Breaking: Major earthquake hits California" ‚Üí U.S. NEWS
- "23 Funniest Tweets About Cats This Week" ‚Üí COMEDY
- "Tips for helping toddler sleep better" ‚Üí PARENTING

Category:"""

        return prompt

    def predict_single(self, headline):
        """Gemma."""
        try:
            prompt = self.create_prompt(headline)

            response = ollama.generate(
                model='gemma:2b',
                prompt=prompt,
                options={
                    'temperature': 0.1,
                    'top_p': 0.9,
                    'num_predict': 10
                }
            )

            predicted_category = response['response'].strip().upper()

            if predicted_category not in self.categories:
                predicted_category = self._find_closest_category(predicted_category)

            return {
                'category': predicted_category,
                'confidence': 0.85,
                'probabilities': {predicted_category: 0.85}
            }

        except Exception as e:
            print(f"‚ùå Error con Gemma: {e}")
            return {
                'category': 'U.S. NEWS',
                'confidence': 0.5,
                'probabilities': {'U.S. NEWS': 0.5}
            }

    def _find_closest_category(self, prediction):
        """Encuentra la categor√≠a m√°s cercana."""
        prediction_lower = prediction.lower()

        if 'news' in prediction_lower:
            if 'u.s' in prediction_lower or 'us' in prediction_lower:
                return 'U.S. NEWS'
            else:
                return 'WORLD NEWS'
        elif 'comedy' in prediction_lower or 'funny' in prediction_lower:
            return 'COMEDY'
        elif 'parent' in prediction_lower:
            return 'PARENTING'
        elif 'sport' in prediction_lower:
            return 'SPORTS'
        elif 'business' in prediction_lower:
            return 'BUSINESS'
        else:
            return 'U.S. NEWS'  # Default

# ==================== 7. EVALUACI√ìN ====================

def evaluate_model(model, X_test, y_test, model_name):
    """
    Esta funcion sirve para evaluar el modelo y generar metricas o insights

    Args:
        model: Modelo entrenado
        X_test: Datos de prueba
        y_test: Etiquetas reales
        model_name: Nombre del modelo
    """
    print(f"\n{'='*50}")
    print(f"EVALUACI√ìN: {model_name}")
    print(f"{'='*50}")

    if hasattr(model, 'predict'):
        predictions, probabilities = model.predict(X_test)
    else:
        predictions = []
        for headline in X_test:
            pred = model.predict_single(headline)
            predictions.append(pred['category'])
        predictions = np.array(predictions)

    accuracy = accuracy_score(y_test, predictions)
    print(f"Accuracy: {accuracy:.4f}")

    print(f"Reporte de Clasificaicon:")
    print(classification_report(y_test, predictions))

    cm = confusion_matrix(y_test, predictions)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=np.unique(y_test),
                yticklabels=np.unique(y_test))
    plt.title(f'Confusion Matrix - {model_name}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    return accuracy

# ==================== 8. FUNCI√ìN PRINCIPAL DE PREDICCI√ìN ====================

def predict_headline(headline, models_dict):
    """
    Predice la categor√≠a de un titular usando m√∫ltiples modelos.

    Args:
        headline (str): Titular a clasificar
        models_dict (dict): Diccionario con modelos entrenados

    Returns:
        dict: Predicciones de todos los modelos
    """
    print(f"\nClasificando: '{headline}'")
    print("="*60)

    results = {}

    for model_name, model in models_dict.items():
        try:
            result = model.predict_single(headline)
            results[model_name] = result

            print(f"{model_name}:")
            print(f"    Categor√≠a: {result['category']}")
            print(f"    Confianza: {result['confidence']:.3f}")

        except Exception as e:
            print(f" Error en {model_name}: {e}")
            results[model_name] = {'category': 'ERROR', 'confidence': 0.0}

    return results

# ==================== 9. EJECUCI√ìN PRINCIPAL ====================

def main():
    """Funci√≥n principal que ejecuta todo el pipeline."""

    print("üöÄ INICIANDO NEWS CLASSIFIER")
    print("="*60)

    df = load_and_explore_data('/content/data.json')
    X_train, X_test, y_train, y_test = prepare_data(df)
    categories = df['category'].unique().tolist()

    print(f"\n{'='*60}")
    print("ENTRENANDO MODELOS")
    print(f"{'='*60}")

    # Modelo 1: TF-IDF
    tfidf_model = TfidfClassifier()
    tfidf_model.train(X_train, y_train)

    # Modelo 2: Sentence-Transformers
    st_model = SentenceTransformerClassifier()
    st_model.train(X_train, y_train)

    # Modelo 3: Gemma
    gemma_model = GemmaClassifier(categories)

    # 4. Evaluaci√≥n de modelos
    models_dict = {
        'TF-IDF + LogReg': tfidf_model,
        'Sentence-Transformers': st_model,
        'Gemma LLM': gemma_model
    }

    accuracies = {}
    for name, model in models_dict.items():
        if name != 'Gemma LLM':  # Gemma es muy lento para evaluar todo el test set
            acc = evaluate_model(model, X_test, y_test, name)
            accuracies[name] = acc

    print(f"\n{'='*60}")
    print("RESUMEN DE RESULTADOS")
    print(f"{'='*60}")

    for model_name, accuracy in accuracies.items():
        print(f"üìä {model_name}: {accuracy:.4f}")

# 6. Aqui estoy usando un test para probar algunos headlines y checar que todo este jalando
    test_headlines = [
        "Breaking: Major earthquake hits California coast",
        "23 Funniest Tweets About Cats This Week",
        "Tips for helping your toddler sleep better at night",
        "Tesla stock soars 15% after earnings report",
        "World Cup final draws record viewership"
    ]

    print(f"\n{'='*60}")
    print("EJEMPLOS DE PREDICCI√ìN")
    print(f"{'='*60}")

    for headline in test_headlines:
        predict_headline(headline, models_dict)

    return models_dict, accuracies

if __name__ == "__main__":
    models, results = main()

